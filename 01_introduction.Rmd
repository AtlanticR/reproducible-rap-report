# INTRODUCTION

The ocean and environmental sciences are becoming increasingly interdisciplinary and technology-driven, which requires researchers to develop new approaches to summarize, handle, and disseminate the vast amounts of information being collected [@Baumann2016; @Farley2018; @Jasny2011; @Sandve2013]. Despite these recent technological advances, the pressure to publish has led to an entrenched individual-focused work culture and reduced collaboration [@Obradovic2019; @Staples2019]. This has resulted in a lack of time, and incentives, to create reproducible work, yet reproducibility is a key tenet of the scientific process [@Heesen2018; @Leek2015; @Munafo2020]. In particular, the digital transformation has drawn attention to the importance of computational reproducibility, or the ability to attain consistent results with a dataset using the same code and methods [@Leek2015; @Peng2011]. This is critical for advancing scientific work by allowing researchers to more effectively build off existing knowledge while minimizing duplication of effort [@Boland2017; @McKiernan2016; @Wolkovich2012]. With mounting human pressures on the marine environment, scientists face a growing sense of urgency to quickly and accurately study these complex systems [@Baumann2016; @Lowndes2017].

The Science Branch within Fisheries and Oceans Canada (DFO) spans a range of diverse research topics and operations, yet is overwhelmed by many of the same issues related to transparent, transferable, and reproducible workflows affecting science globally. Developing reproducible tools, particularly those that focus on data discovery and reporting, were identified as a potential remedy to these siloed work environments [@Edwards2018; @Gomez2021]. In response to these needs, different teams within DFO have developed open-source software tools to address these problems:

* The Pacific Region has created multiple R-based tools for reproducible reporting including ones to generate technical reports and Canadian Science Advisory Secretariat (CSAS) documents [i.e., the *csasdown* R package, @Anderson2022, <https://github.com/pbs-assess/csasdown>]. For example, *csasdown* was used to create an automated Research Document to model the populations of 113 groundfish species [@Anderson2022, see links within <https://github.com/pbs-assess>].
* The Newfoundland and Labrador Region has developed interactive dashboards in support of stock assessment processes [@Regular2020, <https://github.com/PaulRegular/interactive-stock-assessment>].
* The Maritimes Region developed a reproducible atlas technical report in 2012 to model the population status, important habitat, temperature and salinity preferences for 104 fish and invertebrate species [@Ricard2013]. This atlas is currently being updated [@Ricard2021, <https://github.com/dfo-gulf-science/Maritimes-SUMMER-Atlas>] and a similar approach is currently underway in the Gulf Region. Further, a collaborative framework was developed to assess and monitor Marine Protected Areas, where all data assimilation and associated methods were encoded in R (Choi et al. 2018, <https://github.com/jae0>).

Projects and decisions within DFO can be controversial, complex, and resource-intensive [@Doubleday1997; @DFO2018]. Continued advancement of reproducible reporting tools allows for increased efficiency, quality, and transparency by making workflows both repeatable and publicly available [@Lowndes2017; @Munafo2017].

The Strategic Science Planning and Program Integrity division in DFO Maritimes supports various projects and presents many opportunities to modernize data management and reporting practices. In 2018, requests to the division to identify and summarize the available DFO and non-DFO datasets within the Maritimes region were becoming increasingly (frequent). Due to a need for swift and effective approaches, a team of self-proclaimed Strategic Reproducible Analytical Pipeline (RAP) Champions was formed to automate the creation of these Reports. The primary objective of this initiative has been to develop a web-based tool to generate Reproducible Reports to identify and describe DFO and non-DFO datasets within a user-defined area. Specifically, we address internal requests that support processes that provide frequent and standardized advice, such as CSAS, Aquaculture Siting Responses, and Environmental Response, which typically focus on Species at Risk. We have encountered multiple challenges regarding data storage, access, and duplication of effort; therefore, a broad, secondary objective has been to spearhead discussions within DFO to advance reproducible workflows and improve data management practices, aligned with Open Government mandates to make information more accessible to everyone [@DFO2020Strategy; @Gomez2021; @PCO2018; @SCC2021]. This technical report provides an overview of the work done so far to create this Spatial Reproducible Reporting Tool including a description of the workflow and code, lessons learned, and future directions. We present a snapshot of the progress made in the hopes that these efforts, while fulfilling a specific reporting need, will also facilitate increased collaboration and reproducibility in monitoring and research relevant to decision-making within DFO. 

Review:

General notes on introduction: The authors provide a detailed description of the issues surrounding reproducibility in the scientific process with recent technological advances, how this relates to the work carried out in the Science Branch of DFO, provide a breakdown of other tools developed within DFO to help with reproducibility issues, and provide the rationalization for creating the Reproducible Reporting tool outlined in this technical report. In general, the introduction was very well referenced, provided a good background in the area, and explained the issues and potential fixes in reproducible reporting very well. Below I provide some potential edits to the introduction by line numbers, present in the pdf, but also highlighted with () brackets to alert the authours to the sections I reference if the editing will be done via github collaboration. This will be the method I use for potential edits through the rest of the document as well.

Finnis: Jake, thank you for the time and the thoughtful review for this Technical Report. I agree with your comments about the tone. My understanding is that the tone of Technical Reports is slightly open-ended, but I agree it could be made more formal.

Line 129: Is it possible to get a rough number of requests in here? If this is possible, including it could really drive home just how much time the RR tool can save

Finnis: Good idea, thank you! I will add this to Abstract, and the first paragraph of the Discussion. I think those are the best places to include the "Results".


Line 129 - 132: I really enjoy this sentence and if it's appropriate absolutely keep it but this could maybe be reworded to something like "Due to the need for swift and effective approaches the Strategic Reproducable Analytical Pipeline (RAP) Champions team was formed to automate the creation of these reports" That being said, I've only ever reviewed one stock update and a Res. Doc. both of which had a different tone so I'm uncertain if any change needs to be made here.

Finnis: I agree with your edit. I've rephrased this.
\pagebreak
